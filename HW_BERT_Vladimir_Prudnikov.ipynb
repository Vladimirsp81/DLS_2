{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_BERT_Vladimir_Prudnikov.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vladimirsp81/DLS_2/blob/master/HW_BERT_Vladimir_Prudnikov.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO_wCxS951bT",
        "colab_type": "text"
      },
      "source": [
        "# Homework\n",
        "\n",
        "Привет! В этой домашнем задании ты научишься обучении модели BERT. На семинаре был разобран код модели, здесь же посмотрим на то, как надо обработать данные, чтобы на них модель могла учиться. \n",
        "\n",
        "Замечания по выполнению задания:\n",
        "\n",
        "- Код внутри блока `<DON'T TOUCH THIS!>` используется для проверки задания, его нельзя трогать. \n",
        "\n",
        "- Внутри блока `<YOUR CODE>` может больше кода, чем там показано изначально.\n",
        "\n",
        "- От задания требуется написания небольшого отчета в конце.\n",
        "\n",
        "\n",
        "Для начала загрузи нужные библиотеки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dq4NkRu_u3i",
        "colab_type": "code",
        "outputId": "5194c32f-8027-49a1-df08-00001051b006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1360
        }
      },
      "source": [
        "!pip install transformers catalyst"
      ],
      "execution_count": 446,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: catalyst in /usr/local/lib/python3.6/dist-packages (20.4.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.13.1)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (from catalyst) (2.0)\n",
            "Requirement already satisfied: deprecation in /usr/local/lib/python3.6/dist-packages (from catalyst) (2.1.0)\n",
            "Requirement already satisfied: plotly>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from catalyst) (4.4.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from catalyst) (4.1.2.30)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from catalyst) (3.13)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from catalyst) (1.5.0+cu101)\n",
            "Requirement already satisfied: crc32c>=1.7 in /usr/local/lib/python3.6/dist-packages (from catalyst) (2.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from catalyst) (7.0.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from catalyst) (2.4.1)\n",
            "Requirement already satisfied: pandas>=0.22 in /usr/local/lib/python3.6/dist-packages (from catalyst) (1.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from catalyst) (0.22.2.post1)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from catalyst) (0.6.0+cu101)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.6/dist-packages (from catalyst) (0.16.2)\n",
            "Requirement already satisfied: GitPython>=2.1.11 in /usr/local/lib/python3.6/dist-packages (from catalyst) (3.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from catalyst) (20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catalyst) (3.2.1)\n",
            "Requirement already satisfied: tensorboard>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from catalyst) (2.2.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from catalyst) (5.5.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.16.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->catalyst) (3.10.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly>=4.1.0->catalyst) (1.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->catalyst) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22->catalyst) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22->catalyst) (2.8.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->catalyst) (1.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->catalyst) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->catalyst) (2.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=2.1.11->catalyst) (4.0.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->catalyst) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst) (1.2.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (1.28.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (0.9.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (1.6.0.post3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (46.1.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (0.34.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (4.3.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (2.1.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (4.4.2)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.1.11->catalyst) (3.0.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst) (1.3.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (0.2.8)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->catalyst) (0.1.9)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->catalyst) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->catalyst) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlHnoGZN6OKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import sys\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, RandomSampler, Dataset\n",
        "\n",
        "import transformers\n",
        "\n",
        "from catalyst.dl import SupervisedRunner\n",
        "from catalyst.dl.callbacks import AccuracyCallback, SchedulerCallback, F1ScoreCallback\n",
        "from catalyst.utils import set_global_seed, prepare_cudnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmlqBOSO7a5L",
        "colab_type": "text"
      },
      "source": [
        "Внизу идет технический код, который нужен для загрузки датасетов. Его можно уменьшить, выбрав только некоторые из них. Для того, что бы зачесть задание, надо выбрать не менее двух задач, для хотя бы одной из которых нужно использовать два предложения(ответ и вопрос, два предложения и прочее). Подробнее про датасеты [здесь](https://gluebenchmark.com/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4H6_6zgnfRQR",
        "colab": {}
      },
      "source": [
        "TASKS = [\"CoLA\", \"SST\", \"MRPC\", \"QQP\", \"STS\", \"MNLI\", \"SNLI\", \"QNLI\", \"RTE\", \"WNLI\"]\n",
        "TASK2PATH = {\n",
        "    \"CoLA\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FCoLA.zip?alt=media&token=46d5e637-3411-4188-bc44-5809b5bfb5f4\",\n",
        "    \"SST\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8\",\n",
        "    \"MRPC\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc\",\n",
        "    \"QQP\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP.zip?alt=media&token=700c6acf-160d-4d89-81d1-de4191d02cb5\",\n",
        "    \"STS\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSTS-B.zip?alt=media&token=bddb94a7-8706-4e0d-a694-1109e12273b5\",\n",
        "    \"MNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&token=50329ea1-e339-40e2-809c-10c40afff3ce\",\n",
        "    \"SNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSNLI.zip?alt=media&token=4afcfbb2-ff0c-4b2d-a09a-dbf07926f4df\",\n",
        "    \"QNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLIv2.zip?alt=media&token=6fdcf570-0fc5-4631-8456-9505272d1601\",\n",
        "    \"RTE\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb\",\n",
        "    \"WNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FWNLI.zip?alt=media&token=068ad0a0-ded7-4bd7-99a5-5e00222e0faf\",\n",
        "}\n",
        "\n",
        "MRPC_TRAIN = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\"\n",
        "MRPC_TEST = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt\"\n",
        "\n",
        "data_dir = \"data/\"\n",
        "max_seq_length = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O8Y-go7czun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_and_extract(task, data_dir):\n",
        "    print(\"Downloading and extracting %s...\" % task)\n",
        "    data_file = \"%s.zip\" % task\n",
        "    urllib.request.urlretrieve(TASK2PATH[task], data_file)\n",
        "    with zipfile.ZipFile(data_file) as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "    os.remove(data_file)\n",
        "    print(\"\\tCompleted!\")\n",
        "\n",
        "def format_mrpc(data_dir, path_to_data):\n",
        "    print(\"Processing MRPC...\")\n",
        "    mrpc_dir = os.path.join(data_dir, \"MRPC\")\n",
        "    if not os.path.isdir(mrpc_dir):\n",
        "        os.mkdir(mrpc_dir)\n",
        "    if path_to_data:\n",
        "        mrpc_train_file = os.path.join(path_to_data, \"msr_paraphrase_train.txt\")\n",
        "        mrpc_test_file = os.path.join(path_to_data, \"msr_paraphrase_test.txt\")\n",
        "    else:\n",
        "        print(\"Local MRPC data not specified, downloading data from %s\" % MRPC_TRAIN)\n",
        "        mrpc_train_file = os.path.join(mrpc_dir, \"msr_paraphrase_train.txt\")\n",
        "        mrpc_test_file = os.path.join(mrpc_dir, \"msr_paraphrase_test.txt\")\n",
        "        urllib.request.urlretrieve(MRPC_TRAIN, mrpc_train_file)\n",
        "        urllib.request.urlretrieve(MRPC_TEST, mrpc_test_file)\n",
        "    assert os.path.isfile(mrpc_train_file), \"Train data not found at %s\" % mrpc_train_file\n",
        "    assert os.path.isfile(mrpc_test_file), \"Test data not found at %s\" % mrpc_test_file\n",
        "    urllib.request.urlretrieve(TASK2PATH[\"MRPC\"], os.path.join(mrpc_dir, \"dev_ids.tsv\"))\n",
        "\n",
        "    dev_ids = []\n",
        "    with open(os.path.join(mrpc_dir, \"dev_ids.tsv\"), encoding=\"utf8\") as ids_fh:\n",
        "        for row in ids_fh:\n",
        "            dev_ids.append(row.strip().split(\"\\t\"))\n",
        "\n",
        "    with open(mrpc_train_file, encoding=\"utf8\") as data_fh, open(\n",
        "        os.path.join(mrpc_dir, \"train.tsv\"), \"w\", encoding=\"utf8\"\n",
        "    ) as train_fh, open(os.path.join(mrpc_dir, \"dev.tsv\"), \"w\", encoding=\"utf8\") as dev_fh:\n",
        "        header = data_fh.readline()\n",
        "        train_fh.write(header)\n",
        "        dev_fh.write(header)\n",
        "        for row in data_fh:\n",
        "            label, id1, id2, s1, s2 = row.strip().split(\"\\t\")\n",
        "            if [id1, id2] in dev_ids:\n",
        "                dev_fh.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (label, id1, id2, s1, s2))\n",
        "            else:\n",
        "                train_fh.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (label, id1, id2, s1, s2))\n",
        "\n",
        "    with open(mrpc_test_file, encoding=\"utf8\") as data_fh, open(\n",
        "        os.path.join(mrpc_dir, \"test.tsv\"), \"w\", encoding=\"utf8\"\n",
        "    ) as test_fh:\n",
        "        header = data_fh.readline()\n",
        "        test_fh.write(\"index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n\")\n",
        "        for idx, row in enumerate(data_fh):\n",
        "            label, id1, id2, s1, s2 = row.strip().split(\"\\t\")\n",
        "            test_fh.write(\"%d\\t%s\\t%s\\t%s\\t%s\\n\" % (idx, id1, id2, s1, s2))\n",
        "    print(\"\\tCompleted!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4hNWuZ5okuj",
        "colab_type": "code",
        "outputId": "d0e0fb7f-bd20-40b6-f45d-d8b5670c2454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "TASKS = ['SST', 'RTE'] # Или можно просто сюда вписать те датасеты, которые ты выбрал.\n",
        "\n",
        "for task in TASKS:\n",
        "    if task == \"MRPC\":\n",
        "        format_mrpc(data_dir, None)\n",
        "    else:\n",
        "        download_and_extract(task, data_dir)"
      ],
      "execution_count": 450,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading and extracting SST...\n",
            "\tCompleted!\n",
            "Downloading and extracting RTE...\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5S3iSoK8LOG",
        "colab_type": "text"
      },
      "source": [
        "Загрузи один из выбранных датасет с помощью Pandas(не обязательно через него, но так проще) и посмотри на него."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G55yNw5LqDDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBn6ejxaokw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#------------------------------------------------Для датасета RTE\n",
        "# Вместо test-а возьмите valid, а valid сделай из train.\n",
        "\n",
        "# <YOUR CODE>\n",
        "le = LabelEncoder()\n",
        "\n",
        "train_valid_pd = pd.read_csv('/content/data/RTE/train.tsv', sep='\\t').dropna()\n",
        "train_pd_sen1, valid_pd_sen1 = train_valid_pd['sentence1'][0: int(len(train_valid_pd)*0.7)], train_valid_pd['sentence2'][int(len(train_valid_pd)*0.7) : ]\n",
        "train_pd_sen2, valid_pd_sen2 = train_valid_pd['sentence2'][0: int(len(train_valid_pd)*0.7)], train_valid_pd['sentence2'][int(len(train_valid_pd)*0.7) : ]\n",
        "train_pd_labels, valid_pd_labels = train_valid_pd['label'][0: int(len(train_valid_pd)*0.7)], train_valid_pd['label'][int(len(train_valid_pd)*0.7) : ]\n",
        "\n",
        "# Сбрасываем индексы после разделения\n",
        "train_pd_sen1.reset_index(drop=True, inplace=True)\n",
        "train_pd_sen2.reset_index(drop=True, inplace=True)\n",
        "valid_pd_sen1.reset_index(drop=True, inplace=True)\n",
        "valid_pd_sen2.reset_index(drop=True, inplace=True)\n",
        "train_pd_labels.reset_index(drop=True, inplace=True)\n",
        "valid_pd_labels.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Кодируем метки\n",
        "train_pd_labels = le.fit_transform(train_pd_labels)\n",
        "valid_pd_labels = le.fit_transform(valid_pd_labels)\n",
        "\n",
        "test_pd = pd.read_csv('/content/data/RTE/dev.tsv', sep='\\t').dropna()\n",
        "test_pd_sen1, test_pd_sen2, test_pd_labels = test_pd['sentence1'], test_pd['sentence2'], test_pd['label']\n",
        "\n",
        "# Кодируем метки\n",
        "test_pd_labels = le.fit_transform(test_pd_labels)\n",
        "# </YOUR CODE>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZWWdCJYI4iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -----------------------------Для датасета SST\n",
        "\n",
        "# # Вместо test-а возьмите valid, а valid сделай из train.\n",
        "\n",
        "# # <YOUR CODE>\n",
        "# train_valid_pd = pd.read_csv('/content/data/SST-2/train.tsv', sep='\\t')\n",
        "# train_pd, valid_pd = train_valid_pd['sentence'][0: int(len(train_valid_pd)*0.7)], train_valid_pd['sentence'][int(len(train_valid_pd)*0.7) : ]\n",
        "# train_pd_labels, valid_pd_labels = train_valid_pd['label'][0: int(len(train_valid_pd)*0.7)], train_valid_pd['label'][int(len(train_valid_pd)*0.7) : ]\n",
        "# train_pd.reset_index(drop=True, inplace=True)\n",
        "# valid_pd.reset_index(drop=True, inplace=True)\n",
        "# train_pd_labels.reset_index(drop=True, inplace=True)\n",
        "# valid_pd_labels.reset_index(drop=True, inplace=True)\n",
        "# test_pd = pd.read_csv('/content/data/SST-2/dev.tsv', sep='\\t')\n",
        "# test_pd, test_pd_labels = test_pd['sentence'], test_pd['label']\n",
        "# # </YOUR CODE>\n",
        "\n",
        "# test_pd_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK5iar5f8e_7",
        "colab_type": "text"
      },
      "source": [
        "Для начала рассмотрим важную часть обработки текста для трансфомера(и не только) – токенайзер.\n",
        "\n",
        "В качестве примера токенайзера воспользуемся внутренним из библиотеки transformers, обученным для BERT-а. Посмотрим, что он умеет."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I3h53-DpofT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'bert-base-uncased'\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0BjZAx68v6T",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим, как происходит токенизация предложения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDkvs3uQpsCY",
        "colab_type": "code",
        "outputId": "af3433d7-afb3-4dce-d702-8e9485d19b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_sentence = \"Hide new secretions from the parental units.\"\n",
        "print(tokenizer.tokenize(test_sentence))"
      ],
      "execution_count": 455,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hide', 'new', 'secret', '##ions', 'from', 'the', 'parental', 'units', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La_1q8Cj82e8",
        "colab_type": "text"
      },
      "source": [
        "Видно, что предложения разделяются не на слова, а подслова. Токены, которые надо объеденить в слова для получения \"нормального\" текста, выделены с помощью `##`. Посмотрим, как различаются коды токенов с этим символом и без него."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKd0SwME9bi3",
        "colab_type": "code",
        "outputId": "9820dc2b-570b-4676-d833-71dda9e97c10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(tokenizer.convert_tokens_to_ids(['ions']))\n",
        "print(tokenizer.convert_tokens_to_ids(['##ions']))"
      ],
      "execution_count": 456,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[15956]\n",
            "[8496]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5qZXD5Z-D5o",
        "colab_type": "text"
      },
      "source": [
        "Для токенизации предложений воспользуемся методом `encode`. Она принимает предложение как строку или список токенов**(!)**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUZQ6tMNpsEh",
        "colab_type": "code",
        "outputId": "62b7cd29-fb47-4b0b-bfc4-eb4e4c70f5bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tokenizer.encode(test_sentence))"
      ],
      "execution_count": 457,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 1012, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiVCb6RG-Yi2",
        "colab_type": "text"
      },
      "source": [
        "Добавились специальные токены впереди и сзади предложения. Посмотрим на весь список специальных токенов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AqoVIrktk4v",
        "colab_type": "code",
        "outputId": "eef4ccba-a1c1-4b9a-91f8-e43a2a46f083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(tokenizer.special_tokens_map)\n",
        "print({i: j for i, j in zip(tokenizer.all_special_tokens, tokenizer.all_special_ids)})"
      ],
      "execution_count": 458,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
            "{'[PAD]': 0, '[MASK]': 103, '[UNK]': 100, '[CLS]': 101, '[SEP]': 102}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TA4XbeA-phM",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим, что ещё может делать токенайзер. Что требуется нам для обучения BERT-а: добавить паддинг, получить маску аттеншена и тип токенов. Попробуем сделать это самостоятельно и посмотрим, как это сделать с помощью токенайзера.\n",
        "\n",
        "Выбери два предложения из обучающей выборки. Получи их токены с помощью метода `tokenize`. Объедени списки токенов так, чтобы модель могла различать, что они от разных предложений. \n",
        "\n",
        "(Подсказка: на семинаре была картинка с эмбеддингами. Она может подсказать, что надо изменить в токенах предложения) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4esxYXTipsG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <YOUR CODE>\n",
        "s1, s2 = train_pd_sen1[0], train_pd_sen1[3]\n",
        "tokenized_s1, tokenized_s2 = tokenizer.tokenize(s1), tokenizer.tokenize(s2)\n",
        "s_union = tokenized_s1 + [tokenizer.sep_token] + tokenized_s2\n",
        "# </YOUR CODE>\n",
        "\n",
        "# <DON'T TOUCH THIS!>\n",
        "assert tokenizer.encode(s_union) == tokenizer.encode(s1, s2), \"Not equal\"\n",
        "# </DON'T TOUCH THIS!>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k6nrFRHBJBl",
        "colab_type": "text"
      },
      "source": [
        "Теперь надо добавь нулей в полученный список чисел, чтобы они легко складывались в батчи."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY-xd0_qp9rJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <YOUR CODE>\n",
        "u_token = tokenizer.encode(s_union)\n",
        "encoded_full = u_token + [0] * (max_seq_length - len(u_token))\n",
        "# </YOUR CODE>\n",
        "\n",
        "# <DON'T TOUCH THIS!>\n",
        "encoded_correct = tokenizer.encode(s1, s2, max_length=max_seq_length, pad_to_max_length=True)\n",
        "assert len(encoded_full) == len(encoded_correct), \"Different length\"\n",
        "assert encoded_full == encoded_correct, \"Not equal\"\n",
        "# </DON'T TOUCH THIS!>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd5yNVukBZuC",
        "colab_type": "text"
      },
      "source": [
        "В модель также надо кинуть маску для механизма внимания и тип предложения для каждого токена. Сделай их."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hiljf3_vQ75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <YOUR CODE>\n",
        "s1_enc_len = len(tokenizer.encode(tokenized_s1))\n",
        "s2_enc_len = len(tokenizer.encode(tokenized_s2))\n",
        "s1_enc_len, s2_enc_len\n",
        "token_type_ids = [0] * s1_enc_len + [1] * (s2_enc_len - 1) + [0] * (len(encoded_full) - s1_enc_len - s2_enc_len + 1)\n",
        "attention_mask = [1] * (s1_enc_len + s2_enc_len - 1) + [0] * (len(encoded_full) - s1_enc_len - s2_enc_len + 1)\n",
        "# </YOUR CODE>\n",
        "\n",
        "# <DON'T TOUCH THIS!>\n",
        "# encoded_plus = tokenizer.encode_plus(train['sentence'][0], text_pair=train['sentence'][1], max_length=max_seq_length, pad_to_max_length=True)\n",
        "encoded_plus = tokenizer.encode_plus(s1, s2, max_length=max_seq_length, pad_to_max_length=True)\n",
        "assert len(token_type_ids) == len(encoded_plus['token_type_ids']), \"Different length in token_type_ids\"\n",
        "assert token_type_ids == encoded_plus['token_type_ids'], \"Not equal token_type_ids\"\n",
        "assert len(attention_mask) == len(encoded_plus['attention_mask']), \"Different length in attention_mask\"\n",
        "assert attention_mask == encoded_plus['attention_mask'], \"Not equal attention_mask\"\n",
        "# </DON'T TOUCH THIS!>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxruIU08CEgg",
        "colab_type": "text"
      },
      "source": [
        "Как видно из тестов, все нужные для обработки текста для BERT-а вещи может делать токенизатор из `transformers`. Но не все токенизаторы настолько функциональны. Их (почти)полный список:\n",
        "- [Sentence Piece](https://github.com/google/sentencepiece/)\n",
        "- [fastBPE](https://github.com/glample/fastBPE)\n",
        "- [Hugging Face Tokenizers](https://github.com/huggingface/tokenizers)\n",
        "- [YouTokenToMe](https://github.com/VKCOM/YouTokenToMe)\n",
        "\n",
        "Их сравнивают [здесь](https://github.com/VKCOM/YouTokenToMe/blob/master/benchmark.md) или [здесь](https://towardsdatascience.com/a-small-timing-experiment-on-the-new-tokenizers-library-a-write-up-7caab6f80ea6). Также специальные токенайзеры, которые специализируются на \"незападные\" языки. Но не будем на них останавливаться.\n",
        "\n",
        "Теперь ты знаешь достаточно, чтобы написать обработчик данных. Что надо сделать: получить из данных предложения, закодировать их, получить аттенш маску и тип токенов, не забыть про таргет. \n",
        "\n",
        "P.S. Есть более быстрая версия токенизатора для BERT внутри `transformers`, `BertTokenizerFast`. \n",
        "\n",
        "P.S.S. Теперь надо использовать только функционал токенайзера для кодирования предложений, без велосипедов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y-wgeHHokz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, data, labels, tokenizer, data2=None):\n",
        "        self.data = data\n",
        "        self.data2 = data2\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # <YOUR CODE>\n",
        "        self.input_ids = []\n",
        "        self.attention_mask = []\n",
        "        self.token_types_ids = []\n",
        "        self.targets = []\n",
        "\n",
        "        for i in range(self.data.shape[0]):\n",
        "          if self.data2 is None:\n",
        "            encoded_plus = tokenizer.encode_plus(self.data[i], max_length=max_seq_length, pad_to_max_length=True)\n",
        "          else:\n",
        "            encoded_plus = tokenizer.encode_plus(self.data[i], self.data2[i], max_length=max_seq_length, pad_to_max_length=True)\n",
        "          self.input_ids.append(encoded_plus['input_ids'])\n",
        "          self.attention_mask.append(encoded_plus['attention_mask'])\n",
        "          self.token_types_ids.append(encoded_plus['token_type_ids'])\n",
        "          self.targets.append(labels[i])\n",
        "        # </YOUR CODE>\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(self.token_types_ids[idx], dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[idx], dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFVoCAkiFGJt",
        "colab_type": "text"
      },
      "source": [
        "Воспользуйтесь семинаром и построй модель для классификации предложений.\n",
        "\n",
        "(Подсказка: весь код BERT-а из семинара доступен из библиотеки `transformers`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifYaEVd6_664",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertConfig\n",
        "from transformers import BertModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa6coCLFFSZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertForSequenceClassification(nn.Module):\n",
        "    def __init__(self, pretrained_model_name: str, num_labels: int):\n",
        "        super().__init__()\n",
        "\n",
        "        # <YOUR CODE>\n",
        "        config = BertConfig.from_pretrained(pretrained_model_name)\n",
        "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        # </YOUR CODE>\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
        "\n",
        "        # <YOUR CODE>\n",
        "        bert_output = self.bert(input_ids=input_ids,\n",
        "                                attention_mask=attention_mask,\n",
        "                                token_type_ids=token_type_ids)\n",
        "        hidden_state = bert_output[0]\n",
        "        pooled_output = hidden_state[:, 0]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        probas = logits[:, 0]\n",
        "        # </YOUR CODE>\n",
        "\n",
        "        return logits, probas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI9IMsE0G1gh",
        "colab_type": "text"
      },
      "source": [
        "Выбери из [списка](https://huggingface.co/models?search=google%2Fbert_) несколько моделей, которые ты будешь обучать. Сравни их качество на выбранных датасетах. \n",
        "\n",
        "Лучше всего будет выбрать одну основную конфигурацию, и другие с небольшим изменением. Например, пройтись по такой сетке: `{'layers': [2, 4], 'num_heads': [2, 4]}`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-22wBwrFMob",
        "colab_type": "code",
        "outputId": "eebceb6e-aa5e-4300-efbe-7e58e356ac3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# <YOUR CODE>\n",
        "num_labels = 2\n",
        "pretrained_model_name = 'google/bert_uncased_L-2_H-128_A-2'\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(pretrained_model_name)\n",
        "model = BertForSequenceClassification(pretrained_model_name, num_labels=num_labels)\n",
        "# </YOUR CODE>\n",
        "\n",
        "model.to(device)\n",
        "print(\"Success!\")"
      ],
      "execution_count": 465,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCL7wstMczw9",
        "colab_type": "code",
        "outputId": "fcee94bd-7811-4161-ccb2-83ba79aa0c39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "# <YOUR CODE>\n",
        "train_dataset = TextClassificationDataset(data=train_pd_sen1, data2=train_pd_sen2, labels=train_pd_labels, tokenizer=tokenizer)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "valid_dataset = TextClassificationDataset(data=valid_pd_sen1, data2=valid_pd_sen2, labels=valid_pd_labels, tokenizer=tokenizer)\n",
        "valid_sampler = RandomSampler(valid_dataset)\n",
        "valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=batch_size)\n",
        "\n",
        "test_dataset = TextClassificationDataset(data=test_pd_sen1, data2=test_pd_sen2, labels=test_pd_labels, tokenizer=tokenizer)\n",
        "test_sampler = RandomSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "dataloaders = {\n",
        "    'train': train_dataloader,\n",
        "    'valid': valid_dataloader,\n",
        "    'test': test_dataloader\n",
        "}\n",
        "# </YOUR CODE>"
      ],
      "execution_count": 466,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.42 s, sys: 0 ns, total: 2.42 s\n",
            "Wall time: 2.42 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VsyoAmwjnb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 404\n",
        "set_global_seed(seed)\n",
        "prepare_cudnn(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "38ojXge_Hy71",
        "colab": {}
      },
      "source": [
        "# Гиперпараметры для обучения модели. Подбери нужные для каждой модели.\n",
        "\n",
        "epochs = 10\n",
        "lr = 3e-4\n",
        "warmup_steps = len(train_dataloader) // 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3TVwcWzH1ok",
        "colab_type": "text"
      },
      "source": [
        "Добавь Loss, Optimizer и Scheduler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mifqwFYdjnWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer_grouped_parameters = [\n",
        "    {\"params\": [p for n, p in model.named_parameters()], \"weight_decay\": 0.0},\n",
        "]\n",
        "\n",
        "# <YOUR CODE>\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=lr)\n",
        "scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs\n",
        ")\n",
        "# </YOUR CODE>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbbWyPiE7MgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_dir = 'logs/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTXx8X69IClJ",
        "colab_type": "text"
      },
      "source": [
        "Для обучения модели воспользуемся библиотекой `catalyst`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFZ9z53VE5tB",
        "colab_type": "code",
        "outputId": "4f51f8e2-eaf9-4313-ef2d-55a80865392e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        }
      },
      "source": [
        "%%time\n",
        "runner = SupervisedRunner(\n",
        "    input_key=(\n",
        "        \"input_ids\",\n",
        "        \"attention_mask\",\n",
        "        \"token_type_ids\"\n",
        "    ),\n",
        "    output_key = ('logits',\n",
        "                  'probas'\n",
        "    ),\n",
        "    input_target_key = 'targets'\n",
        ")\n",
        "\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    loaders=dataloaders,\n",
        "    callbacks=[\n",
        "        AccuracyCallback(num_classes=num_labels,\n",
        "                         input_key = 'targets',\n",
        "                         output_key = 'logits'),\n",
        "        F1ScoreCallback(input_key = 'targets',\n",
        "                        output_key = 'probas',\n",
        "                        threshold = 0.5\n",
        "                        ),\n",
        "        SchedulerCallback(mode='batch'),\n",
        "    ],\n",
        "    logdir=log_dir,\n",
        "    num_epochs=epochs,\n",
        "    verbose=False,\n",
        ")"
      ],
      "execution_count": 471,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning:\n",
            "\n",
            "To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2020-05-06 20:12:24,151] \n",
            "1/10 * Epoch 1 (_base): lr=0.000e+00 | momentum=0.9000\n",
            "1/10 * Epoch 1 (train): accuracy01=0.4748 | f1_score=0.6262 | loss=0.7674 | lr=7.091e-05 | momentum=0.9000\n",
            "1/10 * Epoch 1 (valid): accuracy01=0.5289 | f1_score=0.6448 | loss=0.6992\n",
            "1/10 * Epoch 1 (test): accuracy01=0.5428 | f1_score=0.6251 | loss=0.6944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning:\n",
            "\n",
            "Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2020-05-06 20:12:25,928] \n",
            "2/10 * Epoch 2 (_base): lr=0.000e+00 | momentum=0.9000\n",
            "2/10 * Epoch 2 (train): accuracy01=0.5263 | f1_score=0.6458 | loss=0.7137 | lr=0.000e+00 | momentum=0.9000\n",
            "2/10 * Epoch 2 (valid): accuracy01=0.5214 | f1_score=0.6546 | loss=0.7010\n",
            "2/10 * Epoch 2 (test): accuracy01=0.5392 | f1_score=0.6286 | loss=0.6992\n",
            "[2020-05-06 20:12:27,515] \n",
            "3/10 * Epoch 3 (_base): lr=0.000e+00 | momentum=0.9000\n",
            "3/10 * Epoch 3 (train): accuracy01=0.5193 | f1_score=0.6485 | loss=0.7111 | lr=0.000e+00 | momentum=0.9000\n",
            "3/10 * Epoch 3 (valid): accuracy01=0.5239 | f1_score=0.6465 | loss=0.7001\n",
            "3/10 * Epoch 3 (test): accuracy01=0.5355 | f1_score=0.6326 | loss=0.6999\n",
            "[2020-05-06 20:12:28,993] \n",
            "4/10 * Epoch 4 (_base): lr=0.000e+00 | momentum=0.9000\n",
            "4/10 * Epoch 4 (train): accuracy01=0.5271 | f1_score=0.6431 | loss=0.7094 | lr=0.000e+00 | momentum=0.9000\n",
            "4/10 * Epoch 4 (valid): accuracy01=0.5289 | f1_score=0.6450 | loss=0.6990\n",
            "4/10 * Epoch 4 (test): accuracy01=0.5392 | f1_score=0.6342 | loss=0.6970\n",
            "[2020-05-06 20:12:30,895] \n",
            "5/10 * Epoch 5 (_base): lr=0.000e+00 | momentum=0.9000\n",
            "5/10 * Epoch 5 (train): accuracy01=0.5305 | f1_score=0.6528 | loss=0.7173 | lr=0.000e+00 | momentum=0.9000\n",
            "5/10 * Epoch 5 (valid): accuracy01=0.5140 | f1_score=0.6576 | loss=0.7042\n",
            "5/10 * Epoch 5 (test): accuracy01=0.5410 | f1_score=0.6343 | loss=0.6969\n",
            "[2020-05-06 20:12:35,620] \n",
            "6/10 * Epoch 6 (_base): lr=0.000e+00 | momentum=0.9000\n",
            "6/10 * Epoch 6 (train): accuracy01=0.5139 | f1_score=0.6428 | loss=0.7061 | lr=0.000e+00 | momentum=0.9000\n",
            "6/10 * Epoch 6 (valid): accuracy01=0.5189 | f1_score=0.6514 | loss=0.7016\n",
            "6/10 * Epoch 6 (test): accuracy01=0.5410 | f1_score=0.6367 | loss=0.6972\n",
            "[2020-05-06 20:12:37,189] \n",
            "7/10 * Epoch 7 (_base): lr=0.000e+00 | momentum=0.9000\n",
            "7/10 * Epoch 7 (train): accuracy01=0.5386 | f1_score=0.6374 | loss=0.7153 | lr=0.000e+00 | momentum=0.9000\n",
            "7/10 * Epoch 7 (valid): accuracy01=0.5189 | f1_score=0.6516 | loss=0.7024\n",
            "7/10 * Epoch 7 (test): accuracy01=0.5428 | f1_score=0.6286 | loss=0.6957\n",
            "[2020-05-06 20:12:38,772] \n",
            "8/10 * Epoch 8 (_base): lr=0.000e+00 | momentum=0.9000\n",
            "8/10 * Epoch 8 (train): accuracy01=0.5260 | f1_score=0.6434 | loss=0.7066 | lr=0.000e+00 | momentum=0.9000\n",
            "8/10 * Epoch 8 (valid): accuracy01=0.5289 | f1_score=0.6496 | loss=0.6988\n",
            "8/10 * Epoch 8 (test): accuracy01=0.5374 | f1_score=0.6372 | loss=0.6978\n",
            "[2020-05-06 20:12:40,506] \n",
            "9/10 * Epoch 9 (_base): lr=0.000e+00 | momentum=0.9000\n",
            "9/10 * Epoch 9 (train): accuracy01=0.5392 | f1_score=0.6468 | loss=0.7021 | lr=0.000e+00 | momentum=0.9000\n",
            "9/10 * Epoch 9 (valid): accuracy01=0.5289 | f1_score=0.6404 | loss=0.6983\n",
            "9/10 * Epoch 9 (test): accuracy01=0.5446 | f1_score=0.6292 | loss=0.6956\n",
            "[2020-05-06 20:12:42,220] \n",
            "10/10 * Epoch 10 (_base): lr=0.000e+00 | momentum=0.9000\n",
            "10/10 * Epoch 10 (train): accuracy01=0.5480 | f1_score=0.6406 | loss=0.7127 | lr=0.000e+00 | momentum=0.9000\n",
            "10/10 * Epoch 10 (valid): accuracy01=0.5214 | f1_score=0.6498 | loss=0.7017\n",
            "10/10 * Epoch 10 (test): accuracy01=0.5374 | f1_score=0.6364 | loss=0.6978\n",
            "Top best models:\n",
            "logs/checkpoints/train.9.pth\t0.6983\n",
            "CPU times: user 11.8 s, sys: 3.93 s, total: 15.7 s\n",
            "Wall time: 19.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViZwMS2pIkBv",
        "colab_type": "text"
      },
      "source": [
        "Ииии отчет!\n",
        "\n",
        "Напиши внизу небольшой отчет о проделанной работе. Ожидается сравнение результатов модели с разным количеством голов/слоев на разных датасетах на `test`. Если для оценки качества на датасете используется необычная метрика(не Accuracy или F1), то можно использовать один из них. Было бы круто, если бы вычислялась нужная метрика и она использовалась в отчете.\n",
        "\n",
        "<ТВОЙ ОТЧЕТ>\n",
        "\n",
        "</ТВОЙ ОТЧЕТ>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3PbkgpXbd3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Модель\\Задача\\Метрика      |    lr    | SST\\Acc  ||    lr   | RTE\\Acc | RTE\\F1  |\n",
        "___________________________|__________|__________||_________|_________|_________| \n",
        "bert_uncased_L-2_H-128_A-2 |  3e-4    |  82.14   ||   3e-4  |  53.74  |  63.72  |\n",
        "___________________________|__________|__________||_________|_________|_________|\n",
        "bert_uncased_L-2_H-256_A-4 |  3e-4    |  81.81   ||   7e-5  |  53.54  |  61.88  |\n",
        "___________________________|__________|__________||_________|_________|_________|\n",
        "bert_uncased_L-6_H-128_A-2 |  1e-4    |  80.69   ||   1e-4  |  47.44  |  58.99  |\n",
        "___________________________|__________|__________||_________|_________|_________|\n",
        "bert_uncased_L-6_H-256_A-4 |  1e-4    |  85.16   ||   1e-4  |  52.15  |  64.00  |\n",
        "___________________________|__________|__________||_________|_________|_________|\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}